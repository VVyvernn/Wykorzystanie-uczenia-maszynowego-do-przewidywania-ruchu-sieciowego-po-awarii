{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ncps.torch import LTC\n",
    "from ncps.wirings import AutoNCP\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_percentage_error\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d66363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sort_key_dirs(filename):\n",
    "    match = re.search(r\"(\\d+)_(\\d+)\", filename)\n",
    "    if match:\n",
    "        return (int(match.group(1)), int(match.group(2)))\n",
    "    return (float('inf'), float('inf'))\n",
    "\n",
    "def custom_sort_key_files(filename):\n",
    "    match = re.search(r\"(\\d+)\", filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return float('inf')\n",
    "\n",
    "def load_data_from_txt_folder(folder_path):\n",
    "    data = []\n",
    "    files = sorted(os.listdir(folder_path), key=custom_sort_key_files)\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            df = pd.read_csv(os.path.join(folder_path, file), header=None, delim_whitespace=True)\n",
    "            bitrate = df.iloc[:, 0].values.reshape(-1, 1)\n",
    "            data.append(bitrate)\n",
    "    return data\n",
    "\n",
    "class BitrateTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, input_steps=2, neigh_dict=None):\n",
    "        self.input_steps = input_steps\n",
    "        self.data = data\n",
    "        self.neigh_dict = neigh_dict\n",
    "        self.samples = []\n",
    "        connection_ids = np.arange(82).reshape(-1, 1)\n",
    "\n",
    "        for i in range(input_steps, len(data)):\n",
    "            x = []\n",
    "            for j in range(input_steps, 0, -1):\n",
    "                bitrate = data[i - j]\n",
    "                x_t = np.concatenate([bitrate, connection_ids], axis=1)  \n",
    "                x.append(x_t)\n",
    "            x = np.stack(x, axis=0) \n",
    "            y = data[i].flatten()  \n",
    "            self.samples.append((x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.samples[idx]\n",
    "        x = np.stack([add_neighbour_channels(frame, self.neigh_dict)\n",
    "                  for frame in x], axis=0) \n",
    "        x_t = torch.tensor(x, dtype=torch.float32)  \n",
    "        y_t = torch.tensor(y, dtype=torch.float32) \n",
    "        return x_t, y_t\n",
    "\n",
    "\n",
    "class TrafficPredictionNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim, ltc_units, dense_units):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(82 + 1, embedding_dim, padding_idx=82) \n",
    "        self.ltc_input_size = 1 + embedding_dim\n",
    "        self.ltc_input_size = 1 + (1 + 7) * embedding_dim \n",
    "\n",
    "\n",
    "        self.ltc = LTC(input_size=self.ltc_input_size, units=ltc_units, return_sequences=True)\n",
    "\n",
    "        self.dense1 = nn.Linear(ltc_units, dense_units[0])\n",
    "        self.dense2 = nn.Linear(dense_units[0], dense_units[1])\n",
    "        self.dense3 = nn.Linear(dense_units[1], 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward(self, x, hx=None):\n",
    "        bitrate = x[..., 0].unsqueeze(-1)             \n",
    "        id_inputs = x[..., 1:].long()                 \n",
    "        embedded = self.embedding(id_inputs)          \n",
    "        embedded = embedded.view(*embedded.shape[:3], -1)  \n",
    "\n",
    "        inputs = torch.cat([bitrate, embedded], dim=-1)  \n",
    "        B, S, L, F = inputs.shape\n",
    "        inputs = inputs.view(B, S * L, F)\n",
    "\n",
    "        ltc_out, hx = self.ltc(inputs, hx)\n",
    "        ltc_out = self.tanh(ltc_out)\n",
    "        d1 = self.dense1(ltc_out)\n",
    "        d2 = self.dense2(d1)\n",
    "        out = self.dense3(d2)\n",
    "        out = out.view(B, S, L)\n",
    "        out = out[:, -1, :]  \n",
    "\n",
    "        return out, hx\n",
    "\n",
    "\n",
    "import time\n",
    "def train_model(data_folder, input_steps=2, embedding_dim=16, ltc_units=64,\n",
    "                dense_units=[20,10], lr=1e-3, batch_size=32, epochs=5, neigh_dict=None, test_name=None):\n",
    "    all_data = load_data_from_txt_folder(data_folder)\n",
    "    train_data = all_data[:6000]\n",
    "    dataset = BitrateTimeSeriesDataset(train_data, input_steps, neigh_dict=neigh_dict)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = TrafficPredictionNetwork(embedding_dim=embedding_dim, ltc_units=ltc_units,\n",
    "                                     dense_units=dense_units)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        epoch_start = time.perf_counter()\n",
    "        total_loss = 0.0\n",
    "        for x_batch, y_batch in loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds, _ = model(x_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {ep+1}/{epochs}: Loss = {total_loss/len(loader):.6f}, time = {time.perf_counter() - epoch_start:.2f}s\")\n",
    "    save_model(model, f\"{test_name}.pth\")\n",
    "    return model, all_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1cfd330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_streaks(predictions, real_values, tolerance=0.15):\n",
    "    streak_info = {}\n",
    "    num_nodes = len(predictions[0])\n",
    "\n",
    "    for node in range(num_nodes):\n",
    "        streak = 0\n",
    "        for t in range(len(predictions)):\n",
    "\n",
    "            if abs(predictions[t][node] - real_values[t][node]) <= tolerance * abs(real_values[t][node]):\n",
    "                streak += 1\n",
    "                if streak == 5:\n",
    "                    streak_info[node] = t - 4\n",
    "                    break\n",
    "            else:\n",
    "                streak = 0\n",
    "    return streak_info, len(streak_info)\n",
    "    \n",
    "def plot_predictions(predictions, real_values=None, node=None):\n",
    "    num_nodes = len(predictions[0])\n",
    "    node_idx = node if node is not None else random.randrange(num_nodes)\n",
    "\n",
    "    node_preds = [p[node_idx] for p in predictions]\n",
    "    avg_preds  = [sum(p)/num_nodes for p in predictions]\n",
    "\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(node_preds, '-o', label='Predicted')\n",
    "    if real_values is not None:\n",
    "        node_reals = [r[node_idx] for r in real_values]\n",
    "        plt.plot(node_reals, '-x', label='Real')\n",
    "    plt.title(f'Connection {node_idx} Bitrate')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('Bitrate')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(avg_preds, '-o', label='Avg Predicted')\n",
    "    if real_values is not None:\n",
    "        avg_reals = [sum(r)/num_nodes for r in real_values]\n",
    "        plt.plot(avg_reals, '-x', label='Avg Real')\n",
    "    plt.title('Average Bitrate Across All Connections')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('Average Bitrate')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b46b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_test_data_for_lnn(root_folder, input_steps=2):\n",
    "    subfolders = [f for f in os.listdir(root_folder) if f.startswith(\"processed_7000_\")]\n",
    "    subfolders.sort(key=custom_sort_key_dirs)\n",
    "    all_data = []\n",
    "    for subfolder in subfolders:\n",
    "        data_path = os.path.join(root_folder, subfolder)\n",
    "        print(f\"Loading data from {data_path}\")\n",
    "        data = load_data_from_txt_folder(data_path)\n",
    "        all_data.append(data)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def evaluate_lnn_on_all_sets(\n",
    "    model,\n",
    "    all_test_data, name_of_test,\n",
    "    criterion, neigh_dict,\n",
    "    *, tolerance=0.15, input_steps=5,\n",
    "    base_lr=1e-3, detach=5):\n",
    "    import copy\n",
    "\n",
    "\n",
    "    summary_results, all_results, all_streak_timesteps = [], [], []\n",
    "    initial_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for data_sequence in all_test_data:\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=base_lr)\n",
    "        model.load_state_dict(copy.deepcopy(initial_model_state))\n",
    "        tol_pct, predictions = evaluate_and_update_lnn_simple_new(\n",
    "            model, data_sequence, optimizer, criterion,\n",
    "            neigh_dict=neigh_dict,\n",
    "            tolerance=tolerance, input_steps=input_steps, detach=detach)\n",
    "        print(\"done wtih set\")\n",
    "\n",
    "        real_values = np.stack([data_sequence[t][:,0] for t in range(7000, 7051)])\n",
    "        pred_arr    = np.asarray(predictions)\n",
    "\n",
    "        rmse_per_ts = np.sqrt(((pred_arr - real_values)**2).mean(axis=1)).tolist()\n",
    "        mape_per_ts = (np.abs(pred_arr - real_values) /\n",
    "                       np.clip(np.abs(real_values), 1e-9, None)).mean(axis=1).tolist()\n",
    "\n",
    "        streak_info, _ = check_for_streaks(pred_arr.tolist(),\n",
    "                                           real_values.tolist(), tolerance)\n",
    "        streak_nodes   = set(streak_info)\n",
    "        streak_ts      = list(streak_info.values())\n",
    "\n",
    "        summary_results.append({\n",
    "            \"number_of_streak_nodes\": len(streak_nodes),\n",
    "            \"average_streak_timestep\": float(np.mean(streak_ts)) if streak_ts else None,\n",
    "            \"percentage_within_tolerance\": tol_pct,\n",
    "            \"percentage_nodes_with_streak\": 100.0*len(streak_nodes)/pred_arr.shape[1],\n",
    "            \"rmse_per_timestep\": rmse_per_ts,\n",
    "            \"mape_per_timestep\": mape_per_ts,\n",
    "        })\n",
    "\n",
    "        all_results.append({\"predictions\": pred_arr.tolist(),\n",
    "                            \"real_values\": real_values.tolist()})\n",
    "        all_streak_timesteps.extend(streak_ts)\n",
    "\n",
    "    with open(f\"{name_of_test}_results.json\", \"w\") as fh:\n",
    "        json.dump(all_results, fh, indent=2)\n",
    "\n",
    "    return all_results, summary_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c4c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_lnn_json_results(json_file_path, tolerance=0.15):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        all_results = json.load(f)\n",
    "\n",
    "    all_metrics = []\n",
    "    all_streak_timesteps = []\n",
    "\n",
    "    for result in all_results:\n",
    "        predictions = result[\"predictions\"]\n",
    "        real_values = result[\"real_values\"]\n",
    "\n",
    "        rmse_per_timestep = []\n",
    "        mape_per_timestep = []\n",
    "        for t in range(len(predictions)):\n",
    "            rmse_timestep = []\n",
    "            mape_timestep = []\n",
    "            for node in range(len(predictions[0])):\n",
    "                real = real_values[t][node]\n",
    "                pred = predictions[t][node]\n",
    "                rmse = np.sqrt((real - pred) ** 2)\n",
    "                mape = np.abs((real - pred) / real) * 100 if real != 0 else 0\n",
    "                rmse_timestep.append(float(rmse))\n",
    "                mape_timestep.append(float(mape))\n",
    "            rmse_per_timestep.append(np.mean(rmse_timestep))\n",
    "            mape_per_timestep.append(np.mean(mape_timestep))\n",
    "\n",
    "        streak_info, _ = check_for_streaks(predictions, real_values, tolerance)\n",
    "        streak_timesteps = list(streak_info.values())\n",
    "\n",
    "        total_within_tolerance = sum(\n",
    "            abs(p - r) <= tolerance * abs(r)\n",
    "            for pred_t, real_t in zip(predictions, real_values)\n",
    "            for p, r in zip(pred_t, real_t)\n",
    "        )\n",
    "        total_preds = len(predictions) * len(predictions[0])\n",
    "        pct_within_tol = (total_within_tolerance / total_preds) * 100\n",
    "        pct_nodes_with_streak = (len(streak_info) / len(predictions[0])) * 100\n",
    "        avg_streak_start = np.mean(streak_timesteps) if streak_timesteps else None\n",
    "\n",
    "        all_metrics.append({\n",
    "            \"rmse\": rmse_per_timestep,\n",
    "            \"mape\": mape_per_timestep,\n",
    "            \"pct_within_tol\": pct_within_tol,\n",
    "            \"pct_nodes_with_streak\": pct_nodes_with_streak,\n",
    "            \"avg_streak_start\": avg_streak_start\n",
    "        })\n",
    "        all_streak_timesteps.extend(streak_timesteps)\n",
    "\n",
    "    avg_rmse = np.mean([m[\"rmse\"] for m in all_metrics], axis=0)\n",
    "    avg_mape = np.mean([m[\"mape\"] for m in all_metrics], axis=0)\n",
    "    avg_within_tol = np.mean([m[\"pct_within_tol\"] for m in all_metrics])\n",
    "    avg_nodes_with_streak = np.mean([m[\"pct_nodes_with_streak\"] for m in all_metrics])\n",
    "    avg_streak_time = np.mean(all_streak_timesteps) if all_streak_timesteps else None\n",
    "\n",
    "    print(f\"Avg % within tolerance: {avg_within_tol:.2f}%\")\n",
    "    print(f\"Avg % of nodes with streak: {avg_nodes_with_streak:.2f}%\")\n",
    "    print(f\"Avg streak start timestep: {avg_streak_time:.2f}\" if avg_streak_time else \"→ No streaks found.\")\n",
    "\n",
    "    timesteps = list(range(len(avg_rmse)))\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(timesteps, avg_rmse, label='Average RMSE', marker='o')\n",
    "    plt.title('Average RMSE per Timestep')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(timesteps, avg_mape, label='Average MAPE', marker='o')\n",
    "    plt.title('Average MAPE per Timestep')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('MAPE')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a836f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_timestep_data(file_path):\n",
    "    df = pd.read_csv(file_path, sep=r'\\s+', header=None, names=[\"bitrate\", \"connection_id\"])\n",
    "    return df['bitrate'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753acfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_hidden_state(h):\n",
    "    if h is None:\n",
    "        return None\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    if isinstance(h, (list, tuple)):\n",
    "        return type(h)(detach_hidden_state(x) for x in h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d64b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_and_update_lnn_simple_new(\n",
    "        model, full_data, optimizer, criterion, *,\n",
    "        neigh_dict,\n",
    "        test_start=6000, test_end=7100,\n",
    "        eval_window=(6999, 7050), incremental_step=20,\n",
    "        tolerance=0.15, input_steps=5, detach=5):\n",
    "\n",
    "    model.train()\n",
    "    h = None\n",
    "    predictions = []\n",
    "    total_loss = 0.0\n",
    "    within_tol = 0\n",
    "    total_eval = 0\n",
    "\n",
    "    eval_start, eval_end = eval_window\n",
    "    connection_ids = np.arange(82).reshape(-1, 1)\n",
    "    detach_every = detach\n",
    "    buf_x, buf_y = [], []\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    for t in range(test_start + input_steps, test_end):\n",
    "        x_seq = []\n",
    "        for j in range(input_steps, 0, -1):\n",
    "            bitrate = full_data[t - j]\n",
    "            frame = np.concatenate([bitrate, connection_ids], axis=1)\n",
    "            x_seq.append(frame)\n",
    "        x_seq = np.stack(x_seq, axis=0)\n",
    "        x_seq = np.stack([add_neighbour_channels(f, neigh_dict) for f in x_seq], axis=0)\n",
    "\n",
    "        x_t = torch.from_numpy(x_seq).float().unsqueeze(0)    \n",
    "        y_true = torch.from_numpy(full_data[t].flatten()).float()\n",
    "\n",
    "        step_idx = t - (test_start + input_steps)\n",
    "        in_eval = (eval_start <= t <= eval_end)\n",
    "\n",
    "        with torch.set_grad_enabled(in_eval):\n",
    "            pred, h = model(x_t, h)\n",
    "\n",
    "        if (step_idx % detach_every == 0) and (h is not None):\n",
    "            h = h.detach()\n",
    "\n",
    "        if in_eval:\n",
    "            loss = criterion(pred.view(-1), y_true)\n",
    "            total_loss += loss.item()\n",
    "            predictions.append(pred.view(-1).tolist())\n",
    "\n",
    "            for p, r in zip(pred.view(-1).tolist(), y_true.tolist()):\n",
    "                if abs(p - r) <= tolerance * abs(r):\n",
    "                    within_tol += 1\n",
    "                total_eval += 1\n",
    "        buf_x.append(x_t.squeeze(0))  \n",
    "        buf_y.append(y_true)\n",
    "        \n",
    "    duration = time.perf_counter() - start_time\n",
    "    avg_loss = total_loss / max((eval_end - eval_start + 1), 1)\n",
    "    tol_pct = 100 * within_tol / total_eval if total_eval > 0 else 0.0\n",
    "\n",
    "    print(f\"Eval complete: loss={avg_loss:.6f}, within_tol={tol_pct:.2f}%, time={duration:.2f}s\")\n",
    "    return tol_pct, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20abad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, List\n",
    "\n",
    "def _enumerate_links(matrix: np.ndarray) -> Tuple[Dict[Tuple[int, int], int], Dict[int, Tuple[int, int]]]:\n",
    "    id2link = {}\n",
    "    link2id = {}\n",
    "    idx = 0\n",
    "    n = matrix.shape[0]\n",
    "    for u in range(n):\n",
    "        for v in range(n):\n",
    "            if u == v or matrix[u, v] == 0:\n",
    "                continue\n",
    "            link2id[(u, v)] = idx\n",
    "            id2link[idx] = (u, v)\n",
    "            idx += 1\n",
    "    return link2id, id2link\n",
    "\n",
    "\n",
    "def build_neighbor_dict(matrix: np.ndarray, k: int = 7, mode: str = 'out') -> Tuple[Dict[int, List[int]], int, Dict[int, Tuple[int, int]]]:\n",
    "\n",
    "    assert mode in {'out', 'in', 'both'}, \"mode must be 'out', 'in', or 'both'\"\n",
    "\n",
    "    link2id, id2link = _enumerate_links(matrix)\n",
    "    num_links = len(id2link)\n",
    "    PAD_ID = 82 \n",
    "\n",
    "    outlinks = {u: [] for u in range(matrix.shape[0])}\n",
    "    inlinks = {v: [] for v in range(matrix.shape[0])}\n",
    "    for (u, v), lid in link2id.items():\n",
    "        outlinks[u].append(lid)\n",
    "        inlinks[v].append(lid)\n",
    "\n",
    "    neigh: Dict[int, List[int]] = {}\n",
    "    for lid, (u, v) in id2link.items():\n",
    "        nbrs = []\n",
    "        if mode in {'out', 'both'}:\n",
    "            nbrs += [n for n in outlinks[u] if n != lid]\n",
    "        if mode in {'in', 'both'}:\n",
    "            nbrs += [n for n in inlinks[v] if n != lid]\n",
    "        seen = set()\n",
    "        uniq = [n for n in nbrs if not (n in seen or seen.add(n))]\n",
    "        padded = (uniq + [PAD_ID] * k)[:k]\n",
    "        neigh[lid] = padded\n",
    "\n",
    "    return neigh, PAD_ID, id2link\n",
    "\n",
    "\n",
    "def add_neighbour_channels(arr: np.ndarray, neigh_dict: Dict[int, List[int]]) -> np.ndarray:\n",
    "    L = arr.shape[0]\n",
    "    k = len(next(iter(neigh_dict.values())))\n",
    "    neighs = np.empty((L, k), dtype=int)\n",
    "    for i in range(L):\n",
    "        neighs[i] = neigh_dict[int(arr[i, 1])]\n",
    "    return np.concatenate([arr, neighs], axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23399a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_static_feat_template(neigh_dict, k=7):\n",
    "    L = 82\n",
    "    conn_ids = np.arange(L).reshape(-1, 1)\n",
    "    neighbors = np.vstack([neigh_dict[lid] for lid in range(L)])\n",
    "\n",
    "    id_block = np.concatenate([conn_ids, neighbors], axis=1)  \n",
    "    zero_bitrate = np.zeros((L, 1)) \n",
    "\n",
    "    template = np.concatenate([zero_bitrate, id_block], axis=1)\n",
    "    return template\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def save_model(model: torch.nn.Module, path: str | Path) -> None:\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model_cls, checkpoint_path: str | Path, *model_args, **model_kwargs):\n",
    "    model = model_cls(*model_args, **model_kwargs)\n",
    "    state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06987b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mat = np.loadtxt(\"Zbiory danych/matrix.net\", dtype=int)\n",
    "neigh, PAD_ID, id2link = build_neighbor_dict(mat, k=7, mode='out')\n",
    "\n",
    "L = 82\n",
    "sample = np.zeros((L, 2))\n",
    "sample[:, 1] = np.arange(L) \n",
    "\n",
    "sample_aug = add_neighbour_channels(sample, neigh)\n",
    "\n",
    "\n",
    "static_template = make_static_feat_template(neigh, k=7)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f24b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_neigh_table(neigh_dict: Dict[int, List[int]], id2link: Dict[int, Tuple[int, int]], limit: int = 10):\n",
    "    print(\"link_id  (u→v)   neighbours\")\n",
    "    print(\"-\" * 40)\n",
    "    for lid in list(neigh_dict.keys())[:limit]:\n",
    "        u, v = id2link[lid]\n",
    "        print(f\"{lid:3d}     ({u:2d}→{v:2d})   {neigh_dict[lid]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f06860",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'Zbiory danych/polaczenie_bitrate_7000/normalized/processed_7000_0'\n",
    "test_name = \"LNN-historia-sąasiedzi\"\n",
    "root_folder = 'Zbiory danych/polaczenie_bitrate_7000/normalized'\n",
    "history = 3\n",
    "detach_step = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea1b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, all_data = train_model(data_folder, neigh_dict=neigh, input_steps=history, embedding_dim=16, ltc_units=64,\n",
    "                              dense_units=[20, 10], lr=1e-3, batch_size=32, epochs=5, test_name=test_name)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421e7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_pct, preds = evaluate_and_update_lnn_simple_new(\n",
    "    model=model,\n",
    "    full_data=all_data,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    test_start=6000,\n",
    "    test_end=7050,\n",
    "    eval_window=(7000, 7050),\n",
    "    incremental_step=20,\n",
    "    tolerance=0.15,\n",
    "    input_steps=2,\n",
    "    neigh_dict=neigh,\n",
    "    detach= detach_step\n",
    ")\n",
    "\n",
    "print(f\"Within tolerance: {tol_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_vals = [arr.flatten().tolist() for arr in all_data[7000:7050]]\n",
    "print(len(real_vals))\n",
    "streaks, num_streaks = check_for_streaks(preds, real_vals)\n",
    "print(f\"Streaks found in {num_streaks} nodes: {streaks}\")\n",
    "\n",
    "plot_predictions(preds, real_vals)\n",
    "\n",
    "print(f\"Percentage of predictions within tolerance: {tol_pct:.2f}%\")\n",
    "\n",
    "avg_streak_start = (\n",
    "                sum(streaks.values()) / len(streaks) if streaks else float('inf')\n",
    "            )\n",
    "print(f\"Average Streak Start: {avg_streak_start:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2021d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data = get_all_test_data_for_lnn(root_folder, input_steps=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84698277",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_path = test_name + \".pth\"\n",
    "model = load_model(\n",
    "    TrafficPredictionNetwork,\n",
    "    trained_path,\n",
    "    embedding_dim = 16,\n",
    "    ltc_units     = 64,\n",
    "    dense_units   = [20, 10]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e270f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_results, summary = evaluate_lnn_on_all_sets(\n",
    "    model=model,\n",
    "    all_test_data=all_test_data,\n",
    "    name_of_test=test_name,\n",
    "    tolerance=0.15,\n",
    "    neigh_dict=neigh,\n",
    "    criterion=criterion,\n",
    "    input_steps=history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f81ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_within_tolerance = np.mean([r[\"percentage_within_tolerance\"] for r in summary])\n",
    "overall_streak_percentage = np.mean([r[\"percentage_nodes_with_streak\"] for r in summary])\n",
    "avg_streak_start = np.mean([r[\"average_streak_timestep\"] for r in summary if r[\"average_streak_timestep\"] is not None])\n",
    "\n",
    "print(f\"Avg % within tolerance: {overall_within_tolerance:.2f}%\")\n",
    "print(f\"Avg % of nodes with streak: {overall_streak_percentage:.2f}%\")\n",
    "print(f\"Avg streak start timestep: {avg_streak_start:.2f}\")\n",
    "print(f\"Saved results to: {test_name}_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_lnn_json_results(\n",
    "    json_file_path= test_name + \"_results.json\",\n",
    "    tolerance=0.15\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
