{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ncps.torch import LTC\n",
    "from ncps.wirings import AutoNCP\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_percentage_error\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d66363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sort_key_dirs(filename):\n",
    "    match = re.search(r\"(\\d+)_(\\d+)\", filename)\n",
    "    if match:\n",
    "        return (int(match.group(1)), int(match.group(2)))\n",
    "    return (float('inf'), float('inf'))\n",
    "\n",
    "def custom_sort_key_files(filename):\n",
    "    match = re.search(r\"(\\d+)\", filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return float('inf')\n",
    "\n",
    "def load_data_from_txt_folder(folder_path):\n",
    "    data = []\n",
    "    files = sorted(os.listdir(folder_path), key=custom_sort_key_files)\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            df = pd.read_csv(os.path.join(folder_path, file), header=None, delim_whitespace=True)\n",
    "            bitrate = df.iloc[:, 0].values.reshape(-1, 1)\n",
    "            data.append(bitrate)\n",
    "    return data\n",
    "\n",
    "class BitrateTimeSeriesDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 input_steps=2,\n",
    "                 neigh_dict=None,\n",
    "                 rolling_window: int = 100):\n",
    "        self.input_steps = input_steps\n",
    "        self.rolling_window = rolling_window\n",
    "        self.data = data\n",
    "        self.neigh_dict = neigh_dict\n",
    "        self.samples = []\n",
    "        connection_ids = np.arange(82).reshape(-1, 1)\n",
    "\n",
    "        start_idx = max(input_steps, rolling_window - 1)\n",
    "\n",
    "        start_idx = (rolling_window - 1) + input_steps\n",
    "\n",
    "        for i in range(start_idx, len(data)):\n",
    "            x = []\n",
    "            for j in range(input_steps, 0, -1):\n",
    "                t = i - j                       \n",
    "                \n",
    "                bitrate = np.asarray(data[t]).reshape(-1, 1)\n",
    "                avg_window = data[t - rolling_window + 1 : t + 1]\n",
    "                avg_mean   = np.mean(avg_window, axis=0)        \n",
    "                if np.ndim(avg_mean) == 0:                      \n",
    "                    avg100 = np.full_like(bitrate, avg_mean)    \n",
    "                else:\n",
    "                    avg100 = np.asarray(avg_mean).reshape(-1, 1) \n",
    "    \n",
    "                frame = np.concatenate(\n",
    "                    [bitrate, connection_ids, avg100], axis=1)      \n",
    "                x.append(frame)\n",
    "\n",
    "            x = np.stack(x, axis=0)                                 \n",
    "            y = data[i].flatten()                                   \n",
    "            self.samples.append((x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.samples[idx]\n",
    "\n",
    "        x = np.stack([add_neighbour_channels(frame, self.neigh_dict)\n",
    "                      for frame in x], axis=0)\n",
    "\n",
    "        x_t = torch.tensor(x, dtype=torch.float32)\n",
    "        y_t = torch.tensor(y, dtype=torch.float32)\n",
    "        return x_t, y_t\n",
    "\n",
    "\n",
    "class TrafficPredictionNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim, ltc_units, dense_units, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(82 + 1, embedding_dim, padding_idx=82)\n",
    "\n",
    "        self.ltc_input_size = 2 + (1 + 7) * embedding_dim\n",
    "\n",
    "        self.ltc   = LTC(input_size=self.ltc_input_size, units=ltc_units,\n",
    "                         return_sequences=True)\n",
    "        self.dense1 = nn.Linear(ltc_units, dense_units[0])\n",
    "        self.dense2 = nn.Linear(dense_units[0], dense_units[1])\n",
    "        self.dense3 = nn.Linear(dense_units[1], 1)\n",
    "        self.tanh   = nn.Tanh()\n",
    "        self.linear = nn.Identity()\n",
    "\n",
    "    def forward(self, x, hx=None):\n",
    "        B, S, N, _ = x.shape\n",
    "\n",
    "        \n",
    "        bitrate = x[..., 0].unsqueeze(-1)              \n",
    "        avg100  = x[..., 2].unsqueeze(-1)              \n",
    "\n",
    "        id_inputs = torch.cat([                       \n",
    "            x[..., 1].unsqueeze(-1),                  \n",
    "            x[..., 3:]                                \n",
    "        ], dim=-1).long()\n",
    "\n",
    "        embedded = self.embedding(id_inputs)          \n",
    "        embedded = embedded.view(B, S, N, -1)         \n",
    "\n",
    "        inputs = torch.cat([bitrate, avg100, embedded], dim=-1)\n",
    "\n",
    "        inputs = inputs.reshape(B * N, S, -1)         \n",
    "\n",
    "        if hx is not None:\n",
    "            hx = hx.reshape(B * N, -1)\n",
    "\n",
    "        out, hx = self.ltc(inputs, hx)                \n",
    "        out = out[:, -1, :]                           \n",
    "\n",
    "        out = self.tanh(self.dense1(out))\n",
    "        out = self.tanh(self.dense2(out))\n",
    "        out = self.linear(self.dense3(out)).squeeze(-1)  \n",
    "\n",
    "        preds = out.view(B, N)                        \n",
    "        return preds, None if hx is None else hx.view(B, N, -1)\n",
    "\n",
    "\n",
    "def train_model(data_folder, input_steps=2, embedding_dim=16, ltc_units=64,\n",
    "                dense_units=[20,10], lr=1e-3, batch_size=32, epochs=5, neigh_dict=None, test_name=None):\n",
    "    all_data = load_data_from_txt_folder(data_folder)\n",
    "    train_data = all_data[:6000]\n",
    "    dataset = BitrateTimeSeriesDataset(train_data, input_steps, neigh_dict=neigh_dict)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = TrafficPredictionNetwork(embedding_dim=embedding_dim, ltc_units=ltc_units,\n",
    "                                     dense_units=dense_units, output_size=82)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for x_batch, y_batch in loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds, _ = model(x_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {ep+1}/{epochs}: Loss = {total_loss/len(loader):.6f}\")\n",
    "    save_model(model, f\"{test_name}.pth\")\n",
    "    return model, all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_streaks(predictions, real_values, tolerance=0.15):\n",
    "    streak_info = {}\n",
    "    num_nodes = len(predictions[0])\n",
    "\n",
    "    for node in range(num_nodes):\n",
    "        streak = 0\n",
    "        for t in range(len(predictions)):\n",
    "\n",
    "            if abs(predictions[t][node] - real_values[t][node]) <= tolerance * abs(real_values[t][node]):\n",
    "                streak += 1\n",
    "                if streak == 5:\n",
    "                    streak_info[node] = t - 4\n",
    "                    break\n",
    "            else:\n",
    "                streak = 0\n",
    "    return streak_info, len(streak_info)\n",
    "    \n",
    "def plot_predictions(predictions, real_values=None, node=None):\n",
    "    num_nodes = len(predictions[0])\n",
    "    node_idx = node if node is not None else random.randrange(num_nodes)\n",
    "\n",
    "    node_preds = [p[node_idx] for p in predictions]\n",
    "    avg_preds  = [sum(p)/num_nodes for p in predictions]\n",
    "\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(node_preds, '-o', label='Predicted')\n",
    "    if real_values is not None:\n",
    "        node_reals = [r[node_idx] for r in real_values]\n",
    "        plt.plot(node_reals, '-x', label='Real')\n",
    "    plt.title(f'Connection {node_idx} Bitrate')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('Bitrate')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(avg_preds, '-o', label='Avg Predicted')\n",
    "    if real_values is not None:\n",
    "        avg_reals = [sum(r)/num_nodes for r in real_values]\n",
    "        plt.plot(avg_reals, '-x', label='Avg Real')\n",
    "    plt.title('Average Bitrate Across All Connections')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('Average Bitrate')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b46b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_test_data_for_lnn(root_folder, input_steps=2):\n",
    "    subfolders = [f for f in os.listdir(root_folder) if f.startswith(\"processed_7000_\")]\n",
    "    subfolders.sort(key=custom_sort_key_dirs)\n",
    "    all_data = []\n",
    "    for subfolder in subfolders:\n",
    "        data_path = os.path.join(root_folder, subfolder)\n",
    "        print(f\"Loading data from {data_path}\")\n",
    "        data = load_data_from_txt_folder(data_path)\n",
    "        all_data.append(data)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def evaluate_lnn_on_all_sets(\n",
    "    all_test_data, name_of_test,\n",
    "    criterion, static_template,\n",
    "    *, tolerance=0.15, input_steps=5,\n",
    "    base_lr=1e-3, model_path=None):\n",
    "\n",
    "    summary_results, all_results, all_streak_timesteps = [], [], []\n",
    "\n",
    "    for data_sequence in all_test_data:\n",
    "        \n",
    "        model = load_model(\n",
    "            TrafficPredictionNetwork,\n",
    "            trained_path,\n",
    "            embedding_dim = 16,\n",
    "            ltc_units     = 32,\n",
    "            dense_units   = [30, 15],\n",
    "            output_size   = 82\n",
    "        )\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=base_lr)\n",
    "\n",
    "        tol_pct, predictions = evaluate_and_update_lnn_simple(\n",
    "            model, data_sequence, optimizer, criterion,\n",
    "            static_template=static_template,\n",
    "            tolerance=tolerance, input_steps=input_steps)\n",
    "        print(\"done wtih set\")\n",
    "\n",
    "        real_values = np.stack([data_sequence[t][:,0] for t in range(7000, 7051)])\n",
    "        pred_arr    = np.asarray(predictions)\n",
    "\n",
    "        rmse_per_ts = np.sqrt(((pred_arr - real_values)**2).mean(axis=1)).tolist()\n",
    "        mape_per_ts = (np.abs(pred_arr - real_values) /\n",
    "                       np.clip(np.abs(real_values), 1e-9, None)).mean(axis=1).tolist()\n",
    "\n",
    "        streak_info, _ = check_for_streaks(pred_arr.tolist(),\n",
    "                                           real_values.tolist(), tolerance)\n",
    "        streak_nodes   = set(streak_info)\n",
    "        streak_ts      = list(streak_info.values())\n",
    "\n",
    "        summary_results.append({\n",
    "            \"number_of_streak_nodes\": len(streak_nodes),\n",
    "            \"average_streak_timestep\": float(np.mean(streak_ts)) if streak_ts else None,\n",
    "            \"percentage_within_tolerance\": tol_pct,\n",
    "            \"percentage_nodes_with_streak\": 100.0*len(streak_nodes)/pred_arr.shape[1],\n",
    "            \"rmse_per_timestep\": rmse_per_ts,\n",
    "            \"mape_per_timestep\": mape_per_ts,\n",
    "        })\n",
    "\n",
    "        all_results.append({\"predictions\": pred_arr.tolist(),\n",
    "                            \"real_values\": real_values.tolist()})\n",
    "        all_streak_timesteps.extend(streak_ts)\n",
    "\n",
    "    with open(f\"{name_of_test}_results_2.json\", \"w\") as fh:\n",
    "        json.dump(all_results, fh, indent=2)\n",
    "\n",
    "    return all_results, summary_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c4c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_lnn_json_results(json_file_path, tolerance=0.15):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        all_results = json.load(f)\n",
    "\n",
    "    all_metrics = []\n",
    "    all_streak_timesteps = []\n",
    "\n",
    "    for result in all_results:\n",
    "        predictions = result[\"predictions\"]\n",
    "        real_values = result[\"real_values\"]\n",
    "\n",
    "        rmse_per_timestep = []\n",
    "        mape_per_timestep = []\n",
    "        for t in range(len(predictions)):\n",
    "            rmse_timestep = []\n",
    "            mape_timestep = []\n",
    "            for node in range(len(predictions[0])):\n",
    "                real = real_values[t][node]\n",
    "                pred = predictions[t][node]\n",
    "                rmse = np.sqrt((real - pred) ** 2)\n",
    "                mape = np.abs((real - pred) / real) * 100 if real != 0 else 0\n",
    "                rmse_timestep.append(float(rmse))\n",
    "                mape_timestep.append(float(mape))\n",
    "            rmse_per_timestep.append(np.mean(rmse_timestep))\n",
    "            mape_per_timestep.append(np.mean(mape_timestep))\n",
    "\n",
    "        streak_info, _ = check_for_streaks(predictions, real_values, tolerance)\n",
    "        streak_timesteps = list(streak_info.values())\n",
    "\n",
    "        total_within_tolerance = sum(\n",
    "            abs(p - r) <= tolerance * abs(r)\n",
    "            for pred_t, real_t in zip(predictions, real_values)\n",
    "            for p, r in zip(pred_t, real_t)\n",
    "        )\n",
    "        total_preds = len(predictions) * len(predictions[0])\n",
    "        pct_within_tol = (total_within_tolerance / total_preds) * 100\n",
    "        pct_nodes_with_streak = (len(streak_info) / len(predictions[0])) * 100\n",
    "        avg_streak_start = np.mean(streak_timesteps) if streak_timesteps else None\n",
    "\n",
    "        all_metrics.append({\n",
    "            \"rmse\": rmse_per_timestep,\n",
    "            \"mape\": mape_per_timestep,\n",
    "            \"pct_within_tol\": pct_within_tol,\n",
    "            \"pct_nodes_with_streak\": pct_nodes_with_streak,\n",
    "            \"avg_streak_start\": avg_streak_start\n",
    "        })\n",
    "        all_streak_timesteps.extend(streak_timesteps)\n",
    "\n",
    "    avg_rmse = np.mean([m[\"rmse\"] for m in all_metrics], axis=0)\n",
    "    avg_mape = np.mean([m[\"mape\"] for m in all_metrics], axis=0)\n",
    "    avg_within_tol = np.mean([m[\"pct_within_tol\"] for m in all_metrics])\n",
    "    avg_nodes_with_streak = np.mean([m[\"pct_nodes_with_streak\"] for m in all_metrics])\n",
    "    avg_streak_time = np.mean(all_streak_timesteps) if all_streak_timesteps else None\n",
    "\n",
    "    print(f\"Avg % within tolerance: {avg_within_tol:.2f}%\")\n",
    "    print(f\"Avg % of nodes with streak: {avg_nodes_with_streak:.2f}%\")\n",
    "    print(f\"Avg streak start timestep: {avg_streak_time:.2f}\" if avg_streak_time else \"→ No streaks found.\")\n",
    "\n",
    "    timesteps = list(range(len(avg_rmse)))\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(timesteps, avg_rmse, label='Average RMSE', marker='o')\n",
    "    plt.title('Average RMSE per Timestep')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(timesteps, avg_mape, label='Average MAPE', marker='o')\n",
    "    plt.title('Average MAPE per Timestep')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('MAPE')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a836f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_timestep_data(file_path):\n",
    "    df = pd.read_csv(file_path, sep=r'\\s+', header=None, names=[\"bitrate\", \"connection_id\"])\n",
    "    return df['bitrate'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753acfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_hidden_state(h):\n",
    "    if h is None:\n",
    "        return None\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    if isinstance(h, (list, tuple)):\n",
    "        return type(h)(detach_hidden_state(x) for x in h)\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a527d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_update_lnn_simple(\n",
    "        model, full_data, optimizer, criterion, *,\n",
    "        static_template,                 \n",
    "        test_start=6000, test_end=7100,\n",
    "        eval_window=(7000, 7050), incremental_step=20,\n",
    "        tolerance=0.15, input_steps=5,\n",
    "        rolling_window=100):             \n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    h = None\n",
    "    buf_x, buf_y = [], []\n",
    "    total_pred = within_tol = 0\n",
    "    predictions = []\n",
    "    S = input_steps\n",
    "\n",
    "    for t in range(test_start + S, test_end):\n",
    "\n",
    "        frames = []\n",
    "        for j in range(S, 0, -1):\n",
    "            abs_t = t - j                       \n",
    "            frame = static_template.copy()\n",
    "            frame[:, 0] = full_data[abs_t][:, 0]          \n",
    "            start = abs_t - rolling_window + 1\n",
    "            if start < 0:\n",
    "                start = 0                      \n",
    "            window = np.stack(full_data[start:abs_t + 1], axis=0)\n",
    "            frame[:, 2] = window[:, :, 0].mean(axis=0)   \n",
    "            frames.append(frame)  \n",
    "\n",
    "        x_seq_np = np.stack(frames, axis=0)       \n",
    "        x_t   = torch.tensor(x_seq_np, dtype=torch.float32).unsqueeze(0)\n",
    "        y_true = torch.tensor(full_data[t][:, 0], dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred, h = model(x_t, h)\n",
    "            h = detach_hidden_state(h)\n",
    "\n",
    "        if eval_window[0] <= t <= eval_window[1]:\n",
    "            predictions.append(y_pred.view(-1).tolist())\n",
    "            within_tol += ((y_pred.view(-1) - y_true)\n",
    "                           .abs() <= tolerance * y_true.abs()).sum().item()\n",
    "            total_pred += y_true.numel()\n",
    "\n",
    "        buf_x.append(x_t.squeeze(0))              \n",
    "        buf_y.append(y_true)                      \n",
    "\n",
    "        if len(buf_x) == incremental_step:\n",
    "            xb = torch.stack(buf_x)               \n",
    "            yb = torch.stack(buf_y)               \n",
    "\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out, _ = model(xb)                    \n",
    "            loss = criterion(out.view(-1), yb.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.eval()\n",
    "\n",
    "            buf_x.clear(); buf_y.clear(); h = None\n",
    "\n",
    "    pct_tol = 100.0 * within_tol / max(total_pred, 1)\n",
    "    return pct_tol, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20abad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "\n",
    "def _enumerate_links(matrix: np.ndarray) -> Tuple[Dict[Tuple[int, int], int], Dict[int, Tuple[int, int]]]:\n",
    "    id2link = {}\n",
    "    link2id = {}\n",
    "    idx = 0\n",
    "    n = matrix.shape[0]\n",
    "    for u in range(n):\n",
    "        for v in range(n):\n",
    "            if u == v or matrix[u, v] == 0:\n",
    "                continue\n",
    "            link2id[(u, v)] = idx\n",
    "            id2link[idx] = (u, v)\n",
    "            idx += 1\n",
    "    return link2id, id2link\n",
    "\n",
    "\n",
    "def build_neighbor_dict(matrix: np.ndarray, k: int = 7, mode: str = 'out') -> Tuple[Dict[int, List[int]], int, Dict[int, Tuple[int, int]]]:\n",
    "\n",
    "    assert mode in {'out', 'in', 'both'}, \"mode must be 'out', 'in', or 'both'\"\n",
    "\n",
    "    link2id, id2link = _enumerate_links(matrix)\n",
    "    num_links = len(id2link)\n",
    "    PAD_ID = 82  \n",
    "\n",
    "    \n",
    "    outlinks = {u: [] for u in range(matrix.shape[0])}\n",
    "    inlinks = {v: [] for v in range(matrix.shape[0])}\n",
    "    for (u, v), lid in link2id.items():\n",
    "        outlinks[u].append(lid)\n",
    "        inlinks[v].append(lid)\n",
    "\n",
    "    neigh: Dict[int, List[int]] = {}\n",
    "    for lid, (u, v) in id2link.items():\n",
    "        nbrs = []\n",
    "        if mode in {'out', 'both'}:\n",
    "            nbrs += [n for n in outlinks[u] if n != lid]\n",
    "        if mode in {'in', 'both'}:\n",
    "            nbrs += [n for n in inlinks[v] if n != lid]\n",
    "       \n",
    "        seen = set()\n",
    "        uniq = [n for n in nbrs if not (n in seen or seen.add(n))]\n",
    "        padded = (uniq + [PAD_ID] * k)[:k]\n",
    "        neigh[lid] = padded\n",
    "\n",
    "    return neigh, PAD_ID, id2link\n",
    "\n",
    "\n",
    "def add_neighbour_channels(arr: np.ndarray, neigh_dict: Dict[int, List[int]]) -> np.ndarray:\n",
    "    L = arr.shape[0]\n",
    "    k = len(next(iter(neigh_dict.values())))\n",
    "    neighs = np.empty((L, k), dtype=int)\n",
    "    for i in range(L):\n",
    "        neighs[i] = neigh_dict[int(arr[i, 1])]\n",
    "    return np.concatenate([arr, neighs], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23399a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_static_feat_template(neigh_dict, k=7):\n",
    "    L = 82\n",
    "    conn_ids = np.arange(L).reshape(-1, 1)                    \n",
    "    neighbors = np.vstack([neigh_dict[lid] for lid in range(L)])  \n",
    "    zero_bitrate = np.zeros((L, 1))   \n",
    "    zero_avg100  = np.zeros((L, 1))   \n",
    "    template = np.concatenate([zero_bitrate, conn_ids,\n",
    "                               zero_avg100, neighbors], axis=1)\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55329b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_neigh_table(neigh_dict: Dict[int, List[int]], id2link: Dict[int, Tuple[int, int]], limit: int = 10):\n",
    "    print(\"link_id  (u→v)   neighbours\")\n",
    "    print(\"-\" * 40)\n",
    "    for lid in list(neigh_dict.keys())[:limit]:\n",
    "        u, v = id2link[lid]\n",
    "        print(f\"{lid:3d}     ({u:2d}→{v:2d})   {neigh_dict[lid]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(model: torch.nn.Module, path: str | Path) -> None:\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model_cls, checkpoint_path: str | Path, *model_args, **model_kwargs):\n",
    "    model = model_cls(*model_args, **model_kwargs)\n",
    "    state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06987b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mat = np.loadtxt(\"matrix.net\", dtype=int)\n",
    "neigh, PAD_ID, id2link = build_neighbor_dict(mat, k=7, mode='out')\n",
    "\n",
    "L = 82\n",
    "sample = np.zeros((L, 2))\n",
    "sample[:, 1] = np.arange(L)  \n",
    "\n",
    "sample_aug = add_neighbour_channels(sample, neigh)\n",
    "\n",
    "static_template = make_static_feat_template(neigh, k=7) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f024f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'polaczenie_bitrate_7000/normalized/processed_7000_0'  \n",
    "root_folder = 'polaczenie_bitrate_7000/normalized'\n",
    "test_name = \"LNN-metryki\"\n",
    "history = 3\n",
    "epoch_num = 5\n",
    "model, all_data = train_model(data_folder, neigh_dict=neigh, input_steps=history, embedding_dim=16, ltc_units=32,\n",
    "                              dense_units=[30, 15], lr=1e-3, batch_size=32, epochs=epoch_num, test_name=test_name\n",
    "                              )\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e88338",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trained_path = test_name + \".pth\"\n",
    "model = load_model(\n",
    "    TrafficPredictionNetwork,\n",
    "    trained_path,\n",
    "    embedding_dim = 16,\n",
    "    ltc_units     = 32,\n",
    "    dense_units   = [30, 15],\n",
    "    output_size   = 82\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd7b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_pct, preds = evaluate_and_update_lnn_simple(\n",
    "    model=model,\n",
    "    full_data=all_data,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    test_start=6000,\n",
    "    test_end=7050,\n",
    "    eval_window=(7000, 7050),\n",
    "    incremental_step=20,\n",
    "    tolerance=0.15,\n",
    "    input_steps=history,\n",
    "    static_template=static_template, \n",
    ")\n",
    "\n",
    "print(f\"Within tolerance: {tol_pct:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bceaa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_vals = [arr.flatten().tolist() for arr in all_data[7000:7050]]\n",
    "print(len(real_vals))\n",
    "streaks, num_streaks = check_for_streaks(preds, real_vals)\n",
    "print(f\"Streaks found in {num_streaks} nodes: {streaks}\")\n",
    "\n",
    "plot_predictions(preds, real_vals)\n",
    "\n",
    "print(f\"Percentage of predictions within tolerance: {tol_pct:.2f}%\")\n",
    "\n",
    "avg_streak_start = (\n",
    "                sum(streaks.values()) / len(streaks) if streaks else float('inf')\n",
    "            )\n",
    "print(f\"Average Streak Start: {avg_streak_start:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data = get_all_test_data_for_lnn(root_folder, input_steps=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73bb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trained_path = test_name + \".pth\"\n",
    "model = load_model(\n",
    "    TrafficPredictionNetwork,\n",
    "    trained_path,\n",
    "    embedding_dim = 16,\n",
    "    ltc_units     = 32,\n",
    "    dense_units   = [30, 15],\n",
    "    output_size   = 82\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a485be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_results, summary = evaluate_lnn_on_all_sets(\n",
    "    all_test_data=all_test_data,\n",
    "    name_of_test=test_name,\n",
    "    tolerance=0.15,\n",
    "    static_template=static_template,\n",
    "    criterion=criterion,\n",
    "    input_steps=history,\n",
    "    model_path = trained_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c35ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_within_tolerance = np.mean([r[\"percentage_within_tolerance\"] for r in summary])\n",
    "overall_streak_percentage = np.mean([r[\"percentage_nodes_with_streak\"] for r in summary])\n",
    "avg_streak_start = np.mean([r[\"average_streak_timestep\"] for r in summary if r[\"average_streak_timestep\"] is not None])\n",
    "\n",
    "print(f\"Avg % within tolerance: {overall_within_tolerance:.2f}%\")\n",
    "print(f\"Avg % of nodes with streak: {overall_streak_percentage:.2f}%\")\n",
    "print(f\"Avg streak start timestep: {avg_streak_start:.2f}\")\n",
    "print(f\"Saved results to: {test_name}_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f93c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_lnn_json_results(\n",
    "    json_file_path=test_name + '_results.json',\n",
    "    tolerance=0.15\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
